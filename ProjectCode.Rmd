---
title: "Project"
author: "Katy Miles"
date: "`r Sys.Date()`"
output: pdf_document
---
# Implementing Solutions to Label Switching for Bootstrap of EM Estimates

## Project Outline

-   Code, in Rcpp, the proposed stratified bootstrap method [Done]
-   Code, in Rcpp, the proposed separated bootstrap method
-   Code, in Rcpp, the proposed label switching adjustment, and then perform parametric bootstrap.
-   Code, in Rcpp, regular parametric bootstrap method
-   Obtain EM algorithm estimates for simulated data from a chosen mixture mode
-   Compare results through tables and graphs
-   Summarize where certain methods fail, succeed, and possible use cases for each


# Stratified Bootstrap 

```{Rcpp}
#include <RcppArmadillo.h>
// [[Rcpp::depends(RcppArmadillo)]]
using namespace Rcpp;
using namespace arma;

//[[Rcpp::export]]
vec stratBoot(NumericVector sample, NumericVector classes, int iterations) {
  vec theta;
  // Find the cluster proportions
  for (int b = 0; b < iterations; b++ ) {
    int c = unique(classes).size();
    NumericVector c_prop;
    for (int i = 0; i < c; i++) {
      int c_count = 0;
      for (int j = 0; j < classes.size(); j++) {
        if (classes[j] == i) {
          c_count++;
        }
      }
      c_prop.push_back(c_count);
    }
    
    // Create bootstrap clusters
    vec fullSample;
    for (int i = 0; i < c; i++) {
      NumericVector clusterSample;
      for (int j = 0; j < classes.size(); j++) {
        if (classes[j] == i) {
          clusterSample.push_back(sample[j]);
        }
      }
      vec bootSample = Rcpp::sample(clusterSample, c_prop[i], true, R_NilValue);
      fullSample = join_cols(fullSample, bootSample);
      // EM algorithm
      theta = join_cols(theta, EM(full_sample, theta, epsilon));
    }
  }
  return theta;
}
```

```{r}
sample = rnorm(10)
classes = rep(c(0,1), 5)

t = stratBoot(sample, classes)
```

# Seperated Bootstrap 

Our proposal for such problems is to apply bootstrap estimation only
after a reliable valid solution (cluster partition) is found, what we will call the chosen solution
from now. 

- This is equivalent to saying that once an acceptable solution has been found, each
cluster can be considered as a mutually independent population.

- Then, the bootstrap procedure
is performed separately on each cluster of the chosen solution, with the CIs computed using a
single-component model for each generated sample.

- Indeed, this implies that we consider the chosen solution as the best that can be found, and
neglect the fact that another may exist, possibly yielding a better fit to the data. However, a better
fit does not necessarily imply a better and more useful clustering. Therefore, one needs to find
the solution most suitable to the data based on all available information, including that from
statistical criteria and from knowledge of the data. In this case, by applying the bootstrap to the
chosen clustering partition, we do not measure the stability of the solution, but rather isolate the
variability of the parameters for this particular clustering partition. This makes sense, especially
when two bootstrap iterations of a given complex model may be completely incompatible (due
to cluster instability, for instance), resulting in the relabeling strategy being proven wrong.

```{Rcpp}

#include <RcppArmadillo.h>
// [[Rcpp::depends(RcppArmadillo)]]
using namespace Rcpp;
using namespace arma;

namespace helper{
  NumericVector EM(mat y, double mu, double lambda, double p, double epsilon) {
    colvec y_vec = y.col(0);
    int n = y_vec.size();
    double p_up;
    double mu_up;
    double lambda_up;
    double diff = 100;
    while (diff > epsilon) {
      // Delta update
      colvec first = p*lambda*exp(-y*lambda);
      colvec second= (1 - p)*mu*exp(-y*mu);
      mat delta_vec = first / (first + second);
      
      // p update
      p_up = accu(delta_vec) / n;
      
      // mu update
      mu_up = accu(1 - delta_vec) / accu(trans(1 - delta_vec)*y);
    
      // lambda update  
      lambda_up = accu(delta_vec) / accu(trans(delta_vec)*y);
  
      // Update difference
      diff = sum(abs(p_up - p) + abs(mu_up - mu) + abs(lambda_up - lambda)) / 
        sum(abs(p) + abs(mu) + abs(lambda));
  
      // Do updates
      p = p_up;
      mu = mu_up;
      lambda = lambda_up;
    }
    NumericVector toReturn = {mu,lambda,p}; 
    
    return toReturn;
  }
}

//[[Rcpp::export]]
vec sepBoot(NumericVector sample, NumericVector classes, int iterations) {
  NumericVector numClasses = unique(classes);
  for (int c = 0; c < numClasses.size(); c++) {
    for (int b = 0; b < iterations; b++) {
      NumericVector classSample;
      for (int i = 0; i < sample.size(); i++) {
        if (classes[i] == c) {
          classSample.push_back(sample[i]);
        }
      }
      vec bootSample = Rcpp::sample(classSample, classSample.size(), true, R_NilValue);
      theta.join_cols(theta, singleComponent(bootSample, theta, epsilon));
    }
  }
}

```

```{r}
EM()
```

# Label Switching 

```{Rcpp}
#include <RcppArmadillo.h>
// [[Rcpp::depends(RcppArmadillo)]]
using namespace Rcpp;
using namespace arma;

namespace helper{
  mat class_probs(mat y, double mu, double lambda, double p, double epsilon) {
    colvec y_vec = y.col(0);
    int n = y_vec.size();
    double p_up;
    double mu_up;
    double lambda_up;
    double diff = 100;
    while (diff > epsilon) {
      // Delta update
      colvec first = p*lambda*exp(-y*lambda);
      colvec second= (1 - p)*mu*exp(-y*mu);
      mat delta_vec = first / (first + second);
      
      // p update
      p_up = accu(delta_vec) / n;
      
      // mu update
      mu_up = accu(1 - delta_vec) / accu(trans(1 - delta_vec)*y);
    
      // lambda update  
      lambda_up = accu(delta_vec) / accu(trans(delta_vec)*y);
  
      // Update difference
      diff = sum(abs(p_up - p) + abs(mu_up - mu) + abs(lambda_up - lambda)) / 
        sum(abs(p) + abs(mu) + abs(lambda));
      
      
  
      // Do updates
      p = p_up;
      mu = mu_up;
      lambda = lambda_up;
    }

    return delta_vec;
  }

}

//[[Rcpp::export]]
vec complh(NumericVector sample, NumericVector classes) {
  
  mat probs = helper::class_probs(sample, mu, lambda, p, epsilon)
  NumericVector numClasses = unique(classes);
  for (int c = 0; c < numClasses.size(); c++) {
    NumericVector classSample;
    for (int i = 0; i < sample.size(); i++) {
      if (classes[i] == c) {
        classSample.push_back(sample[i]);
      }
    }
    vec bootSample = Rcpp::sample(classSample, classSample.size(), true, R_NilValue);
  }
}

```

# EM algorithm

```{r}
library(tidyverse)
library(emdbook)
data = read.csv("C:/Users/16189/Desktop/STAT 630/mixture.csv")

## EM algorithm

# Step 1: Initialization
p1_em = c(1/3)
p2_em = c(1/3)
p3_em = c(1/3)
mu1_em = c(-1.5, 2)
mu2_em = c(2, 2)
mu3_em = c(-5, 1.5)
sig1_em <- matrix(c(2, 0, 0, 2), nrow = 2) 
sig2_em <- matrix(c(1.5, 0, 0, 2), nrow = 2) 
sig3_em <- matrix(c(1, 0, 0, 2), nrow = 2) 

n = length(data$X1)
y = as.matrix(data)
prev_parameters = 100
diff = 50
while (diff > 0.00001) {
  # E
  w1 = p1_em * dmvnorm(y, mu1_em, sig1_em)
  w2 = p2_em * dmvnorm(y, mu2_em, sig2_em)
  w3 = p3_em * dmvnorm(y, mu3_em, sig3_em)
  w1_tot = w1 / (w1 + w2 + w3)
  w2_tot = w2 / (w1 + w2 + w3)
  w3_tot = w3 / (w1 + w2 + w3)
  
  #M
  mu1_em = apply(w1_tot * y / sum(w1_tot), 2, sum)
  mu2_em = apply(w2_tot * y / sum(w2_tot), 2, sum)
  mu3_em = apply(w3_tot * y / sum(w3_tot), 2, sum)
  dev1 = y - t(matrix(rep(mu1_em, n), ncol = n))
  sig1_em = (t(w1_tot * dev1) %*% dev1)/sum(w1_tot)
  dev2 = y - t(matrix(rep(mu2_em, n), ncol = n))
  sig2_em = (t(w2_tot*dev2) %*% dev2)/sum(w2_tot)
  dev3 = y - t(matrix(rep(mu3_em, n), ncol = n))
  sig3_em = (t(w3_tot*dev3) %*% dev3)/sum(w3_tot)	
  p1_em = sum(w1_tot)/n
  p2_em = sum(w2_tot)/n
  p3_em = sum(w3_tot)/n
  diff = abs(prev_parameters - p1_em)
  prev_parameters = p1_em
}


```

# Simulations

```{r}
simulateData = function(n, lambda, mu, theta, p) {
  data = p[1]*rnorm(n, lambda) + p[2]*rnorm(n, mu) + p[3]*rnorm(n, theta)
  return(as.matrix(data))
}

n = 100
lambda = 1
prob = c(1/3,1/3,1/3)
mu = 2
theta = c(1,2,3)
data_list = list()
for (i in 1:100) {
  data_list = append(data_list, list(simulateData(n, lambda, mu, theta, p)))
}

ggplot() + 
  geom_line(aes(seq(-10,10,.1), dnorm(seq(-10,10,.1), lambda))) + 
  geom_line(aes(seq(-10,10,.1), dnorm(seq(-10,10,.1), mu))) + 
  geom_line(aes(seq(-10,10,.1), dnorm(seq(-10,10,.1), theta))) 

complh = function(n, theta_hat) {
  # Generating sample
  x = numeric(n)
  z = matrix(nrow = 3, ncol = n)
  for (i in 1:n) {
    j = sample(c(1,2,3), 1, prob = prob)
    z[1, i] = ifelse(j == 1, 1, 0)
    z[2, i] = ifelse(j == 2, 1, 0)
    z[3, i] = ifelse(j == 3, 1, 0)
    x[i] = rnorm(1, theta[j])
  }
  
  perm = permn(1:3)
  last = 0
  ratio_sum = numeric(length(perm))
  ratio = numeric(n)
  for (k in 1:length(perm)) {
    p = perm[[k]]
    for (i in 1:n) {
      num = 0
      for (j in 1:3) {
        eval = z[j,i]*log(prob[p[j]]*dnorm(x[i], theta_hat[p[j]]))
        num = num + eval
      }
      ratio[i] = num
    }
    ratio_sum[k] = sum(ratio)
  }
  which.max(ratio_sum)
}

```


