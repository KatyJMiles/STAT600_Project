---
title: "Project"
author: "Katy Miles"
date: "`r Sys.Date()`"
output: html_document
---
# Implementing Solutions to Label Switching for Bootstrap of EM Estimates

## Project Outline

-   Code, in Rcpp, the proposed stratified bootstrap method [Done]
-   Code, in Rcpp, the proposed separated bootstrap method
-   Code, in Rcpp, the proposed label switching adjustment, and then perform parametric bootstrap.
-   Code, in Rcpp, regular parametric bootstrap method
-   Obtain EM algorithm estimates for simulated data from a chosen mixture mode
-   Compare results through tables and graphs
-   Summarize where certain methods fail, succeed, and possible use cases for each

# EM algorithm

```{Rcpp}
#include <RcppArmadillo.h>
// [[Rcpp::depends(RcppArmadillo)]]
using namespace Rcpp;
using namespace arma;

//[[Rcpp::export]]
List em_norm(NumericVector y, NumericVector p, NumericVector theta, 
             bool classes = false) {
  int n = y.size();
  NumericVector w1_tot(n);
  NumericVector w2_tot(n);
  double mu1 = theta[0];
  double mu2 = theta[1];
  double p1 = p[0];
  double p2 = p[1];
  for (int i = 0; i < 50; i++) {
      // E
      NumericVector w1 = p1 * dnorm(y, mu1);
      NumericVector w2 = p2 * dnorm(y, mu2);

      w1_tot = w1 / (w1 + w2);
      w2_tot = w2 / (w1 + w2);
      
      //M
      mu1 = sum(w1_tot * y) / sum(w1_tot);
      mu2 = sum(w2_tot * y) / sum(w2_tot);
      p1 = sum(w1_tot)/n;
      p2 = sum(w2_tot)/n;
  }
  NumericVector p_hat = {p1, p2};
  NumericVector theta_hat = {mu1, mu2};
  if (classes) {
      return List::create(p_hat, theta_hat, w1_tot, w2_tot);
  }
  return List::create(p_hat, theta_hat);
}


namespace helper{
  NumericVector em_helper(NumericVector y, NumericVector p, NumericVector theta, 
               bool classes = false) {
    int n = y.size();
    NumericVector w1_tot(n);
    NumericVector w2_tot(n);
    double mu1 = theta[0];
    double mu2 = theta[1];
    double p1 = p[0];
    double p2 = p[1];
    for (int i = 0; i < 50; i++) {
        // E
        NumericVector w1 = p1 * dnorm(y, mu1);
        NumericVector w2 = p2 * dnorm(y, mu2);
  
        w1_tot = w1 / (w1 + w2);
        w2_tot = w2 / (w1 + w2);
        
        //M
        mu1 = sum(w1_tot * y) / sum(w1_tot);
        mu2 = sum(w2_tot * y) / sum(w2_tot);
        p1 = sum(w1_tot)/n;
        p2 = sum(w2_tot)/n;
    }
    NumericVector ests = {p1, p2, mu1, mu2};
    NumericVector w_tot = cbind(w1_tot, w2_tot);
    if (classes) {
        return cbind(ests, w_tot);
    }
    return ests;
  }
}

//[[Rcpp::export]]
NumericVector stratBoot(NumericVector data, NumericVector classes, 
                        NumericVector params, int iterations) {
  NumericVector theta = params[Range(2,3)];
  NumericVector p = params[Range(0,1)];
  NumericVector result(0);
  int c = p.size();
  // Find the cluster proportions
  for (int b = 0; b < iterations; b++) {
    // Create bootstrap clusters
    NumericVector fullSample(0);
    for (int i = 0; i < c; i++) {
      NumericVector clusterSample(0);
      for (int j = 0; j < classes.size(); j++) {
        if (classes[j] == i + 1) {
          clusterSample.push_back(data[j]);
        }
      }
      if (clusterSample.size() > 0) {
        NumericVector bootSample = sample(clusterSample, p[i]*data.size(), true);
        for (int b = 0; b < bootSample.size(); b++) {
          fullSample.push_back(bootSample[b]);
        }
      }
    }
    
    // EM algorithm
    NumericVector em_eval = helper::em_helper(fullSample, p, theta);
    for (int i = 0; i < em_eval.length(); i++) {
      result.push_back(em_eval[i]);
    }
  }
  return result;
}
```


```{r}
# Stratified bootstrap:  R version
strtBoot = function(data, classes, params, iterations) {
  theta = params[3:4]
  p = params[1:2]
  result = c()
  c = length(p)
  # Find the cluster proportions
  for (b in 1:iterations) {
    # Create bootstrap clusters
    fullSample = c()
    for (i in 1:c) {
      clusterSample = c()
      for (j in 1:length(classes)) {
        if (classes[j] == i) {
          clusterSample = append(clusterSample, data[j])
        }
      }
      if (length(clusterSample) > 0) {
        bootSample = sample(clusterSample, p[i]*length(data), TRUE)
        fullSample = append(fullSample, bootSample)
      }
    }
    
    # EM algorithm
    result = rbind(result, unlist(em_norm(fullSample, p, theta)))
  }
  return(result)
}

regularBoot = function(data, params, iterations) {
  s = data
  theta = matrix(nrow = 0, ncol = 4)
  
  for (j in 1:iterations) {
    # Get EM estimates 
    theta = rbind(theta, unlist(em_norm(s, params[1:2], params[3:4])))
    
    # Increment j
    s = as.matrix(sample(data, length(data), replace = TRUE))
  }
  return(theta)
}
```


```{Rcpp}
#include <RcppArmadillo.h>
#include <bits/stdc++.h> 

// [[Rcpp::depends(RcppArmadillo)]]
using namespace Rcpp;
using namespace arma;
using namespace std;

namespace helper{ 
  NumericMatrix findPermutations(IntegerVector& c, int n) { 
    NumericVector perm(4);
    // Sort the given array 
    sort(c.begin(), c.end()); 
    // Find all possible permutations 
    do {
      for (int i = 0; i < n; i++) { 
        perm.push_back(c[i]); 
      } 
    } while (next_permutation(c.begin(), c.end()));
    NumericMatrix toReturn( c.size() , n , perm.begin() );
    return toReturn;
  }
  arma::vec perm_sort(arma::vec x, arma::vec y) {
      return x(arma::sort_index(y));
  }
}

//[[Rcpp::export]]
vec complh(int n, vec theta_hat, vec p_hat, NumericVector latent_labels) {
  // Generating sample
  int m = theta_hat.size();
  NumericVector x(n);
  NumericMatrix z(m, n);
  IntegerVector classes =  seq(1, m);
  NumericVector prop(m);
  prop[0] = p_hat[0];
  prop[1] = p_hat[1];
  for (int i = 0; i < n; i++) {
    if (latent_labels[i] ==1) {
      z(1,i) = 1;
    } else {
      z(1,i) = 0;
    }
    if (latent_labels[i] == 2) {
      z(2,i) = 1;
    } else {
      z(2,i) = 0;
    }
    x[i] = R::rnorm(theta_hat[latent_labels[i]], 1);
  }
  // Label Adjustment using complete log likelihood
  NumericMatrix perm = helper::findPermutations(classes, m);

  double ratio_sum = -10000000;
  NumericVector ratio(n);
  int max_perm = 0;
  for (int k = 0; k < perm.nrow(); k++) {
    NumericVector p = perm(k , _ );
    for (int i = 0; i < n; i++) {
      int num = 0;
      for (int j = 0; j < m; j++) {
        double mean = theta_hat[p[j]];
        double like = R::dnorm(x[i], mean, 1, 0);
        double eval = z(j,i)*log(p_hat[p[j]]*like);
        num = num + eval;
      }
      ratio[i] = num;
    }
    if (sum(ratio) > ratio_sum) {
      ratio_sum = sum(ratio);
      max_perm = k;
    }
  }
  vec y = perm(max_perm, _);
  vec sorted_p = helper::perm_sort(p_hat, y);
  vec sorted_theta = helper::perm_sort(theta_hat, y);
  return join_cols(sorted_p, sorted_theta);
}

//[[Rcpp::export]]
vec distlat(int n, vec theta_hat, vec p_hat, NumericVector latent_labels) {
  // Generating sample
  int m = theta_hat.size();
  NumericVector x(n);
  NumericMatrix z(m, n);
  IntegerVector classes =  seq(1, m);
  NumericVector prop(m);
  prop[0] = p_hat[0];
  prop[1] = p_hat[1];
  for (int i = 0; i < n; i++) {
    if (latent_labels[i] ==1) {
      z(1,i) = 1;
    } else {
      z(1,i) = 0;
    }
    if (latent_labels[i] == 2) {
      z(2,i) = 1;
    } else {
      z(2,i) = 0;
    }
    x[i] = R::rnorm(theta_hat[latent_labels[i]], 1);
  }
  // Label Adjustment using complete log likelihood
  NumericMatrix perm = helper::findPermutations(classes, m);

  double ratio_sum = -10000000;
  NumericVector ratio(n);
  int max_perm = 0;
  for (int k = 0; k < perm.nrow(); k++) {
    NumericVector p = perm(k , _ );
    for (int i = 0; i < n; i++) {
      int num = 0;
      for (int j = 0; j < m; j++) {
        double mean = theta_hat[p[j]];
        double like = R::dnorm(x[i], mean, 1, 0);
        double eval = z(j,i)*p_hat[p[j]]*like;
        num = num + eval;
      }
      ratio[i] = num;
    }
    if (sum(ratio) > ratio_sum) {
      ratio_sum = sum(ratio);
      max_perm = k;
    }
  }
  vec y = perm(max_perm, _);
  vec sorted_p = helper::perm_sort(p_hat, y);
  vec sorted_theta = helper::perm_sort(theta_hat, y);
  return join_cols(sorted_p, sorted_theta);
}

```

```{r}
fake_data = as.data.frame(matrix(c(x,label), ncol = 2))
colnames(fake_data) = c("Value", "Label")
fake_data = rowid_to_column(fake_data)
fake_data = fake_data %>%
  mutate(Label = as.factor(Label)) 

ggplot() + 
  geom_point(aes(Value, rowid, color = Label), fake_data) +
  theme_minimal() + 
  geom_text(x=3, y=30, label="Log-Likelihood = -412.6700")


fake_data = fake_data %>%
  mutate(Label = ifelse(Label == "1", "2", "1"))
ggplot() + 
  geom_point(aes(Value, rowid, color = Label), fake_data) +
  theme_minimal() + 
  annotate("text", x=8, y=13, label= "Log-Likelihood = -412.6700") 

likelihood = function(x, p, theta) {
  return(sum(log(p[1]*dnorm(x, theta[1]) + p[2]*dnorm(x, theta[2]))))
}
```


```{r}
library(tidyverse)
library(foreach)
library(doParallel)
library(rbenchmark)

simulations = function(true_p, true_theta, n) {
  regList = list()
  stratList = list()
  adjList = list()
  distList = list()
  for (j in 1:100) {
    # Simulate Data
    p_binom = rbinom(n, 1, true_p[1])
    data = (p_binom)*rnorm(n, true_theta[1]) + (1 - p_binom)*rnorm(n, true_theta[2])
    
    # Get initial values from kmeans
    initial_theta = sort(kmeans(data, 2)$centers[,1])
    initial_p = sort(kmeans(data, 2)$size / length(data))
    
    # Perform EM
    em_result = em_norm(data, initial_p, initial_theta, classes = TRUE)
    theta_hat = em_result[[2]]
    p_hat = em_result[[1]]
    
    # Set number of iterations
    iterations = 100
    
    # Perform Stratified Bootstrap
    w2_tot = em_result[[4]]
    w1_tot = em_result[[3]]
    
    df = data.frame(w1_tot, 
               w2_tot)
    df = df %>%
      rowwise() %>%
      mutate(Classification = ifelse(w1_tot == max(w1_tot, w2_tot), 1, 2))
    
    # Perform Non Parametric Bootstrap
    regBoot = regularBoot(data, params = c(p_hat, theta_hat), iterations)
    
    # Perform Parametric Bootstrap with Label Adjustment
    adjBoot = matrix(ncol = 4, nrow = iterations)
    for (i in 1:iterations) {
      # Take sample
      p_binom = rbinom(n, 1, p_hat[1])
      sample = (p_binom)*rnorm(n, theta_hat[1]) + (1 - p_binom)*rnorm(n,theta_hat[2])
      
      # Get estimates
      em = em_norm(sample, p_hat, theta_hat, classes = TRUE)
      
      w2_tot = em[[4]]
      w1_tot = em[[3]]
      
      df = data.frame(w1_tot, 
                 w2_tot)
      df = df %>%
        rowwise() %>%
        mutate(Classification = ifelse(w1_tot == max(w1_tot, w2_tot), 1, 2))
      
      # Label Adjustment
      adjBoot[i,] = complh(n, em[[2]], em[[1]], df$Classification)
    }
  
     # Perform Parametric Bootstrap with Label Adjustment DISTLAT
    distBoot = matrix(ncol = 4, nrow = iterations)
    for (i in 1:iterations) {
      # Take sample
      p_binom = rbinom(n, 1, p_hat[1])
      sample = (p_binom)*rnorm(n, theta_hat[1]) + (1 - p_binom)*rnorm(n,theta_hat[2])
      
      # Get estimates
      em = em_norm(sample, p_hat, theta_hat, classes = TRUE)
      
      w2_tot = em[[4]]
      w1_tot = em[[3]]
      
      df = data.frame(w1_tot, 
                 w2_tot)
      df = df %>%
        rowwise() %>%
        mutate(Classification = ifelse(w1_tot == max(w1_tot, w2_tot), 1, 2))
      
      # Label Adjustment
      distBoot[i,] = distlat(n, em[[2]], em[[1]], df$Classification)
    }
    
    sBoot = strtBoot(data, df$Classification, c(p_hat, theta_hat), iterations)
    regCI = c()
    adjCI = c()
    distCI = c()
    stratCI = c()
    # Get quantile CI
    for (i in 1:4) {
      regCI = rbind(regCI, quantile(regBoot[,i], c(.025,.975)))
      adjCI = rbind(adjCI, quantile(adjBoot[,i], c(.025, .975)))
      distCI = rbind(distCI, quantile(distBoot[,i], c(.025, .975)))
      stratCI = rbind(stratCI, quantile(sBoot[,i], c(.025,.975)))
    }
    regList = append(regList, list(regCI))
    adjList = append(adjList, list(adjCI))
    distList = append(distList, list(distCI))
    stratList = append(stratList, list(stratCI))
  }
    
    regCoverage = numeric(4)
    stratCoverage = numeric(4)
    adjCoverage = numeric(4)
    distCoverage = numeric(4)
    for (j in 1:4) {
      params = c(true_p, true_theta)
      coverage = numeric(100)
      for (i in 1:100) {
        CI = regList[[i]][j,]
        coverage[i] = ifelse(CI[1] < params[j] && CI[2] > params[j], 1, 0)
      }
      regCoverage[j] = sum(coverage)
      
      coverage = numeric(100)
      for (i in 1:100) {
        CI = adjList[[i]][j,]
        coverage[i] = ifelse(CI[1] < params[j] && CI[2] > params[j], 1, 0)
      }
      adjCoverage[j] = sum(coverage)
      
      coverage = numeric(100)
      for (i in 1:100) {
        CI = distList[[i]][j,]
        coverage[i] = ifelse(CI[1] < params[j] && CI[2] > params[j], 1, 0)
      }
      distCoverage[j] = sum(coverage)
      
      coverage = numeric(100)
      for (i in 1:100) {
        CI = stratList[[i]][j,]
        coverage[i] = ifelse(CI[1] < params[j] && CI[2] > params[j], 1, 0)
      }
      stratCoverage[j] = sum(coverage)
    }
    
    table = rbind(regCoverage, rbind(adjCoverage, rbind(distCoverage, stratCoverage)))
    return(table)
}
```

1000 Bootstrap runs, n = 100

Scenario 1: $$p_1 = 0.5, p_2 = 0.5, \mu_1 = 0, mu_2 = 0.5$$

```{r}
table1 = simulations(c(.5,.5), c(0, .5), 100)
colnames(table1) = c("P1", "P2", "Theta1", "Theta2")
knitr::kable(table1)
table1 = cbind(table1, Diff = .5)
```

Scenario 2: $$p_1 = 0.5, p_2 = 0.5, \mu_1 = 0, mu_2 = 1$$

```{r}
table2 = simulations(c(.5,.5), c(0, 1), 100)
colnames(table2) = c("P1", "P2", "Theta1", "Theta2")
knitr::kable(table2)
table2 = cbind(table2, Diff = rep(1, 4))
```

Scenario 3: $$p_1 = 0.5, p_2 = 0.5, \mu_1 = 0, \mu_2 = 1.5$$

```{r}
table3 = simulations(c(.5,.5), c(0, 1.5), 100)
colnames(table3) = c("P1", "P2", "Theta1", "Theta2")
knitr::kable(table3)
table3 = cbind(table3, Diff = rep(1.5, 4))
```

Scenario 4: $$p_1 = 0.5, p_2 = 0.5, \mu_1 = 0, \mu_2 = 2$$

```{r}
table4 = simulations(c(.5,.5), c(0, 2), 100)
colnames(table4) = c("P1", "P2", "Theta1", "Theta2")
knitr::kable(table4)
table4 = cbind(table4, Diff = rep(2, 4))
```

```{r}
plot_df = as.data.frame(rbind(table1, rbind(table2, rbind(table3, table4))))
plot_df = cbind(plot_df, Method = rep(c("Non-Parametric", "COMPLH", "DISTLAT", "Strat"), 4))

plot_simulations = function(param, options, plot_df, title) {
  yvar = sym(options[param])
  print(yvar)
  ggplot() + 
    geom_point(aes(Diff, !!yvar, color = Method), plot_df) + 
    geom_line(aes(Diff, !!yvar, color = Method), plot_df) + 
    theme_minimal() + 
    ggtitle(title) + 
    ylab("Coverage Probability") + 
    theme(plot.title = element_text(hjust = 0.5))
}

plot_simulations(1, c("P1", "P2", "Theta1", "Theta2"), plot_df, "P")
plot_simulations(3, c("P1", "P2", "Theta1", "Theta2"), plot_df, "Lambda 1")
plot_simulations(4, c("P1", "P2", "Theta1", "Theta2"), plot_df, "Lambda 2")
```

1000 Bootstrap runs, n = 50

Scenario 1: $$p_1 = 0.5, p_2 = 0.5, \mu_1 = 0, mu_2 = 0.5$$

```{r}
table1_50 = simulations(c(.5,.5), c(0, .5), 50)
colnames(table1_50) = c("P1", "P2", "Theta1", "Theta2")
knitr::kable(table1_50)
```

Scenario 2: $$p_1 = 0.5, p_2 = 0.5, \mu_1 = 0, mu_2 = 1$$

```{r}
table2_50 = simulations(c(.5,.5), c(0, 1), 50)
colnames(table2_50) = c("P1", "P2", "Theta1", "Theta2")
knitr::kable(table2_50)
```

Scenario 4: $$p_1 = 0.5, p_2 = 0.5, \mu_1 = 0, mu_2 = 2$$

```{r}
table3_50 = simulations(c(.5,.5), c(0, 1.5), 50)
colnames(table3_50) = c("P1", "P2", "Theta1", "Theta2")
knitr::kable(table3_50)
```

Scenario 4: $$p_1 = 0.5, p_2 = 0.5, \mu_1 = 0, mu_2 = 2$$

```{r}
table4_50 = simulations(c(.5,.5), c(0, 2), 50)
colnames(table4_50) = c("P1", "P2", "Theta1", "Theta2")
knitr::kable(table4_50)
```

1000 Bootstrap runs, n = 200

Scenario 1: $$p_1 = 0.3, p_2 = 0.7, \mu_1 = 0, mu_2 = 0.5$$

Scenario 2: $$p_1 = 0.3, p_2 = 0.7, \mu_1 = 0, mu_2 = 1$$

Scenario 3: $$p_1 = 0.3, p_2 = 0.7, \mu_1 = 0, mu_2 = 1.5$$

Scenario 4: $$p_1 = 0.3, p_2 = 0.7, \mu_1 = 0, mu_2 = 2$$
